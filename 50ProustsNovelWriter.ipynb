{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103301"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/swannsway.txt', 'r') as f:\n",
    "    swann = f.read()\n",
    "len(swann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   ! \" $ % \\' ( ) * , - . / 0 1 2 3 4 5 6 7 8 9 : ; ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z _ a b c d e f g h i j k l m n o p q r s t u v w x y z ° Î Ï à â ç è é ê ë î ï ô'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters = sorted(list(set(swann)))\n",
    "\" \".join(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter2int = {letters[i]:i for i in range(len(letters))}\n",
    "int2letters = {i:letters[i] for i in range(len(letters))}\n",
    "swann_encoded = np.array([letter2int[letter] for letter in swann], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103301"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swann_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \n",
    "    ## Let's reshape arr in order to have array of dim: (n_seqs, ??)\n",
    "    batch_total_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_total_size\n",
    "    arr = arr[:(n_batches * batch_total_size)]\n",
    "    arr_reshaped = arr.reshape((n_seqs, -1))\n",
    "    for i in range(n_batches):\n",
    "        x = arr_reshaped[:, i*n_steps:(i+1)*n_steps]\n",
    "        y = np.concatenate([x[:, 1:], x[:,:1]], axis=1)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_bashes = get_batches(swann_encoded, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[42, 49, 32, 45, 47],\n",
       "        [73, 59,  1, 69, 68],\n",
       "        [59, 58,  1, 74, 69],\n",
       "        [74, 55, 63, 68, 73],\n",
       "        [68, 55, 74, 75, 72],\n",
       "        [62, 59, 72,  1, 56],\n",
       "        [59, 10,  1, 58, 59],\n",
       "        [63, 67,  1, 74, 62],\n",
       "        [72, 55, 74, 59,  1],\n",
       "        [ 1, 77, 62, 63, 57]], dtype=int32), array([[49, 32, 45, 47, 42],\n",
       "        [59,  1, 69, 68, 73],\n",
       "        [58,  1, 74, 69, 59],\n",
       "        [55, 63, 68, 73, 74],\n",
       "        [55, 74, 75, 72, 68],\n",
       "        [59, 72,  1, 56, 62],\n",
       "        [10,  1, 58, 59, 59],\n",
       "        [67,  1, 74, 62, 63],\n",
       "        [55, 74, 59,  1, 72],\n",
       "        [77, 62, 63, 57,  1]], dtype=int32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(gen_bashes)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "    keep_probabilites = tf.placeholder(tf.float32, name=\"keeps_probabilites\")\n",
    "    return inputs, targets, keep_probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.5400...  5.3717 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.4331...  5.3046 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8096...  5.4135 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.1092...  6.5687 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 3.9418...  7.0100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8240...  5.7198 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.6653...  5.9794 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.5249...  6.1635 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.3972...  5.8014 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.3798...  5.7329 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3912...  6.0343 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3537...  5.8743 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3464...  5.5788 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3297...  5.4846 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.2831...  5.4037 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.2861...  5.5146 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2465...  6.2592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.2364...  6.0645 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2333...  5.6441 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2181...  5.9455 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2391...  6.0137 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2128...  5.8410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2129...  6.2997 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.1886...  6.0010 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2158...  5.5551 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.1981...  5.5320 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.1968...  5.9089 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2111...  5.8892 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.1968...  6.0687 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2000...  6.1130 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.1639...  6.4390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.1940...  5.4182 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1694...  5.3033 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.1588...  5.3753 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1404...  5.4354 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.1459...  5.4697 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1557...  5.4069 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1321...  5.8293 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1707...  6.1734 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1446...  8.1534 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1379...  6.3698 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1317...  6.9962 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1558...  6.4321 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1635...  6.3314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1360...  5.7616 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1300...  5.5428 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1365...  5.8095 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1352...  5.8739 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1303...  7.6336 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1298...  8.9542 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1257...  5.7170 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1088...  8.3361 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1121...  5.4356 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1193...  7.0482 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1123...  6.4851 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.0961...  5.6314 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1033...  5.9859 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.0941...  5.6086 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1087...  5.6845 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1162...  5.3624 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.0898...  5.3658 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.0952...  5.8719 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1071...  6.8444 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1010...  6.1422 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1290...  6.0386 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1020...  5.9202 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1126...  6.0079 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1196...  6.1563 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1131...  6.2319 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1189...  5.7777 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1253...  5.4484 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1459...  5.5274 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1032...  5.3379 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.0812...  6.4588 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.0942...  7.4281 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.0790...  6.2645 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.0798...  6.2207 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.0424...  5.4286 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.0739...  5.4748 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.0842...  5.3299 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.0849...  5.4171 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1055...  5.3990 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.0829...  5.5612 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.2774...  5.4451 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.2805...  5.7263 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.2827...  6.0772 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.2357...  5.7628 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1955...  7.4790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1581...  6.7438 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.0962...  5.8188 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.0728...  5.5290 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.0600...  5.4956 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.0626...  5.5472 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.0704...  5.8112 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.0704...  6.7995 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.0595...  6.5593 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.0311...  7.1256 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.0416...  7.0918 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.0619...  5.9129 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0703...  6.0271 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.0393...  7.4988 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0611...  6.8869 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0388...  6.2244 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0338...  7.3243 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0445...  6.4038 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0176...  6.5957 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0207...  5.6104 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0380...  5.8634 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0286...  5.7361 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0242...  6.0162 sec/batch\n",
      "Epoch: 2/20...  Training Step: 111...  Training loss: 3.0639...  7.2477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 112...  Training loss: 2.9974...  5.4902 sec/batch\n",
      "Epoch: 2/20...  Training Step: 113...  Training loss: 3.0009...  5.5367 sec/batch\n",
      "Epoch: 2/20...  Training Step: 114...  Training loss: 3.0039...  6.2038 sec/batch\n",
      "Epoch: 2/20...  Training Step: 115...  Training loss: 2.9747...  6.4356 sec/batch\n",
      "Epoch: 2/20...  Training Step: 116...  Training loss: 2.9957...  6.4525 sec/batch\n",
      "Epoch: 2/20...  Training Step: 117...  Training loss: 2.9789...  7.2383 sec/batch\n",
      "Epoch: 2/20...  Training Step: 118...  Training loss: 2.9700...  6.8161 sec/batch\n",
      "Epoch: 2/20...  Training Step: 119...  Training loss: 2.9232...  7.1386 sec/batch\n",
      "Epoch: 2/20...  Training Step: 120...  Training loss: 2.9654...  6.6496 sec/batch\n",
      "Epoch: 2/20...  Training Step: 121...  Training loss: 2.9810...  5.6158 sec/batch\n",
      "Epoch: 2/20...  Training Step: 122...  Training loss: 2.9485...  5.8476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 123...  Training loss: 2.9834...  5.5620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 124...  Training loss: 2.9456...  5.5983 sec/batch\n",
      "Epoch: 2/20...  Training Step: 125...  Training loss: 2.9564...  6.2945 sec/batch\n",
      "Epoch: 2/20...  Training Step: 126...  Training loss: 2.9481...  6.3292 sec/batch\n",
      "Epoch: 2/20...  Training Step: 127...  Training loss: 2.9223...  6.3216 sec/batch\n",
      "Epoch: 2/20...  Training Step: 128...  Training loss: 2.9061...  6.0396 sec/batch\n",
      "Epoch: 2/20...  Training Step: 129...  Training loss: 2.9263...  5.4276 sec/batch\n",
      "Epoch: 2/20...  Training Step: 130...  Training loss: 2.8854...  5.5898 sec/batch\n",
      "Epoch: 2/20...  Training Step: 131...  Training loss: 2.9091...  5.4905 sec/batch\n",
      "Epoch: 2/20...  Training Step: 132...  Training loss: 2.8809...  5.3083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 133...  Training loss: 2.8667...  5.4454 sec/batch\n",
      "Epoch: 2/20...  Training Step: 134...  Training loss: 2.8636...  5.5292 sec/batch\n",
      "Epoch: 2/20...  Training Step: 135...  Training loss: 2.8789...  5.6722 sec/batch\n",
      "Epoch: 2/20...  Training Step: 136...  Training loss: 2.8633...  6.4195 sec/batch\n",
      "Epoch: 2/20...  Training Step: 137...  Training loss: 2.8407...  5.6307 sec/batch\n",
      "Epoch: 2/20...  Training Step: 138...  Training loss: 2.8611...  5.6843 sec/batch\n",
      "Epoch: 2/20...  Training Step: 139...  Training loss: 2.8282...  5.7627 sec/batch\n",
      "Epoch: 2/20...  Training Step: 140...  Training loss: 2.8148...  6.3890 sec/batch\n",
      "Epoch: 2/20...  Training Step: 141...  Training loss: 2.7842...  7.6205 sec/batch\n",
      "Epoch: 2/20...  Training Step: 142...  Training loss: 2.8083...  6.3450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 143...  Training loss: 2.7793...  5.8340 sec/batch\n",
      "Epoch: 2/20...  Training Step: 144...  Training loss: 2.7664...  6.5734 sec/batch\n",
      "Epoch: 2/20...  Training Step: 145...  Training loss: 2.8268...  5.5600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 146...  Training loss: 2.8161...  7.0598 sec/batch\n",
      "Epoch: 2/20...  Training Step: 147...  Training loss: 2.7915...  7.7604 sec/batch\n",
      "Epoch: 2/20...  Training Step: 148...  Training loss: 2.7636...  6.7308 sec/batch\n",
      "Epoch: 2/20...  Training Step: 149...  Training loss: 2.8020...  6.7315 sec/batch\n",
      "Epoch: 2/20...  Training Step: 150...  Training loss: 2.7500...  6.3022 sec/batch\n",
      "Epoch: 2/20...  Training Step: 151...  Training loss: 2.7438...  5.8548 sec/batch\n",
      "Epoch: 2/20...  Training Step: 152...  Training loss: 2.7341...  5.8500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 153...  Training loss: 2.7337...  5.8259 sec/batch\n",
      "Epoch: 2/20...  Training Step: 154...  Training loss: 2.7430...  5.7861 sec/batch\n",
      "Epoch: 2/20...  Training Step: 155...  Training loss: 2.7069...  5.7715 sec/batch\n",
      "Epoch: 2/20...  Training Step: 156...  Training loss: 2.6841...  7.0083 sec/batch\n",
      "Epoch: 2/20...  Training Step: 157...  Training loss: 2.6909...  6.3538 sec/batch\n",
      "Epoch: 2/20...  Training Step: 158...  Training loss: 2.7028...  5.6138 sec/batch\n",
      "Epoch: 2/20...  Training Step: 159...  Training loss: 2.6776...  5.4216 sec/batch\n",
      "Epoch: 2/20...  Training Step: 160...  Training loss: 2.6797...  5.3346 sec/batch\n",
      "Epoch: 2/20...  Training Step: 161...  Training loss: 2.6557...  5.3928 sec/batch\n",
      "Epoch: 2/20...  Training Step: 162...  Training loss: 2.6269...  5.4301 sec/batch\n",
      "Epoch: 2/20...  Training Step: 163...  Training loss: 2.6291...  5.3269 sec/batch\n",
      "Epoch: 2/20...  Training Step: 164...  Training loss: 2.6232...  5.5154 sec/batch\n",
      "Epoch: 2/20...  Training Step: 165...  Training loss: 2.6210...  5.4439 sec/batch\n",
      "Epoch: 2/20...  Training Step: 166...  Training loss: 2.5952...  5.4998 sec/batch\n",
      "Epoch: 2/20...  Training Step: 167...  Training loss: 2.6040...  5.3490 sec/batch\n",
      "Epoch: 2/20...  Training Step: 168...  Training loss: 2.5778...  5.7167 sec/batch\n",
      "Epoch: 2/20...  Training Step: 169...  Training loss: 2.6033...  5.4658 sec/batch\n",
      "Epoch: 2/20...  Training Step: 170...  Training loss: 2.6035...  6.0203 sec/batch\n",
      "Epoch: 2/20...  Training Step: 171...  Training loss: 2.5564...  5.6842 sec/batch\n",
      "Epoch: 2/20...  Training Step: 172...  Training loss: 2.5733...  5.7681 sec/batch\n",
      "Epoch: 2/20...  Training Step: 173...  Training loss: 2.5591...  6.4685 sec/batch\n",
      "Epoch: 2/20...  Training Step: 174...  Training loss: 2.5702...  5.5617 sec/batch\n",
      "Epoch: 2/20...  Training Step: 175...  Training loss: 2.5951...  5.5096 sec/batch\n",
      "Epoch: 2/20...  Training Step: 176...  Training loss: 2.5666...  5.5816 sec/batch\n",
      "Epoch: 2/20...  Training Step: 177...  Training loss: 2.5727...  5.4945 sec/batch\n",
      "Epoch: 2/20...  Training Step: 178...  Training loss: 2.5719...  5.5838 sec/batch\n",
      "Epoch: 2/20...  Training Step: 179...  Training loss: 2.5744...  6.3079 sec/batch\n",
      "Epoch: 2/20...  Training Step: 180...  Training loss: 2.5616...  6.2955 sec/batch\n",
      "Epoch: 2/20...  Training Step: 181...  Training loss: 2.5586...  6.0765 sec/batch\n",
      "Epoch: 2/20...  Training Step: 182...  Training loss: 2.5959...  6.0485 sec/batch\n",
      "Epoch: 2/20...  Training Step: 183...  Training loss: 2.5378...  5.8686 sec/batch\n",
      "Epoch: 2/20...  Training Step: 184...  Training loss: 2.5158...  5.8635 sec/batch\n",
      "Epoch: 2/20...  Training Step: 185...  Training loss: 2.5440...  6.3422 sec/batch\n",
      "Epoch: 2/20...  Training Step: 186...  Training loss: 2.5309...  7.3195 sec/batch\n",
      "Epoch: 2/20...  Training Step: 187...  Training loss: 2.5156...  7.3472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 188...  Training loss: 2.4817...  5.8219 sec/batch\n",
      "Epoch: 2/20...  Training Step: 189...  Training loss: 2.5215...  6.4868 sec/batch\n",
      "Epoch: 2/20...  Training Step: 190...  Training loss: 2.5127...  7.7005 sec/batch\n",
      "Epoch: 2/20...  Training Step: 191...  Training loss: 2.5094...  6.1403 sec/batch\n",
      "Epoch: 2/20...  Training Step: 192...  Training loss: 2.5403...  7.2596 sec/batch\n",
      "Epoch: 2/20...  Training Step: 193...  Training loss: 2.4973...  6.4486 sec/batch\n",
      "Epoch: 2/20...  Training Step: 194...  Training loss: 2.5147...  5.8733 sec/batch\n",
      "Epoch: 2/20...  Training Step: 195...  Training loss: 2.4783...  5.7984 sec/batch\n",
      "Epoch: 2/20...  Training Step: 196...  Training loss: 2.4988...  5.6730 sec/batch\n",
      "Epoch: 2/20...  Training Step: 197...  Training loss: 2.5124...  5.6990 sec/batch\n",
      "Epoch: 2/20...  Training Step: 198...  Training loss: 2.4687...  5.7349 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5134...  5.7980 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4976...  5.5464 sec/batch\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of checkpoints/i200_l512.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-130979656199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_every_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/i{}_l{}.ckpt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/i{}_l{}.ckpt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bartekskorulski/Envs/tf1.0/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1365\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of checkpoints/i200_l512.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(letters), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(swann_encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
